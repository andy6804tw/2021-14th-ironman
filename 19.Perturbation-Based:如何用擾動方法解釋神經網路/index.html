
<!DOCTYPE html>

<html class="no-js" lang="zh-Hant">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="ie=edge" http-equiv="x-ua-compatible"/>
<meta content="10程式中" name="author"/>
<meta content="複製" name="lang:clipboard.copy"/>
<meta content="已複製" name="lang:clipboard.copied"/>
<meta content="ja" name="lang:search.language"/>
<meta content="True" name="lang:search.pipeline.stopwords"/>
<meta content="True" name="lang:search.pipeline.trimmer"/>
<meta content="沒有符合的項目" name="lang:search.result.none"/>
<meta content="找到 1 個符合的項目" name="lang:search.result.one"/>
<meta content="找到 # 個符合的項目" name="lang:search.result.other"/>
<meta content="[\uff0c\u3002]+" name="lang:search.tokenizer"/>
<link href="../assets/images/favicon.png" rel="shortcut icon"/>
<meta content="mkdocs-1.0.4, mkdocs-material-4.4.0" name="generator"/>
<title>[Day 19] Perturbation-Based：如何用擾動方法解釋神經網路 - 全民瘋AI系列 [探索可解釋人工智慧]</title>
<link href="../assets/stylesheets/application.0284f74d.css" rel="stylesheet"/>
<link href="../assets/stylesheets/application-palette.01803549.css" rel="stylesheet"/>
<meta content="#7e57c2" name="theme-color"/>
<script src="../assets/javascripts/modernizr.74668098.js"></script>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&amp;display=fallback" rel="stylesheet"/>
<style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
<link href="../assets/fonts/material-icons.css" rel="stylesheet"/>
<link href="../stylesheets/extra.css" rel="stylesheet"/>
</head>
<body data-md-color-accent="deep-purple" data-md-color-primary="deep-purple" dir="ltr">
<svg class="md-svg">
<defs>
<svg height="448" id="__github" viewbox="0 0 416 448" width="416" xmlns="http://www.w3.org/2000/svg"><path d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z" fill="currentColor"></path></svg>
</defs>
</svg>
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
<a class="md-skip" href="#day-19-perturbation-based" tabindex="1">
        跳轉到
      </a>
<header class="md-header" data-md-component="header">
<nav class="md-header-nav md-grid">
<div class="md-flex">
<div class="md-flex__cell md-flex__cell--shrink">
<a class="md-header-nav__button md-logo" href=".." title="全民瘋AI系列 [探索可解釋人工智慧]">
<i class="md-icon"></i>
</a>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
</div>
<div class="md-flex__cell md-flex__cell--stretch">
<div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
<span class="md-header-nav__topic">
              全民瘋AI系列 [探索可解釋人工智慧]
            </span>
<span class="md-header-nav__topic">
              
                [Day 19] Perturbation-Based：如何用擾動方法解釋神經網路
              
            </span>
</div>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="query" data-md-state="active" name="query" placeholder="搜尋" spellcheck="false" type="text"/>
<label class="md-icon md-search__icon" for="__search"></label>
<button class="md-icon md-search__icon" data-md-component="reset" tabindex="-1" type="reset">
        
      </button>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="result">
<div class="md-search-result__meta">
            打字進行搜尋
          </div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<div class="md-header-nav__source">
<a class="md-source" data-md-source="github" href="https://github.com/andy6804tw/2023-15th-ironman" title="前往倉庫">
<div class="md-source__icon">
<svg height="24" viewbox="0 0 24 24" width="24">
<use height="24" width="24" xlink:href="#__github"></use>
</svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
</div>
</div>
</nav>
</header>
<div class="md-container">
<main class="md-main">
<div class="md-main__inner md-grid" data-md-component="container">
<div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title md-nav__title--site" for="__drawer">
<a class="md-nav__button md-logo" href=".." title="全民瘋AI系列 [探索可解釋人工智慧]">
<i class="md-icon"></i>
</a>
    全民瘋AI系列 [探索可解釋人工智慧]
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-source="github" href="https://github.com/andy6804tw/2023-15th-ironman" title="前往倉庫">
<div class="md-source__icon">
<svg height="24" viewbox="0 0 24 24" width="24">
<use height="24" width="24" xlink:href="#__github"></use>
</svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" id="nav-1" type="checkbox"/>
<label class="md-nav__link" for="nav-1">
      1.XAI基礎與概念介紹
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-1">
        1.XAI基礎與概念介紹
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../1.揭開模型的神秘面紗:為何XAI對機器學習如此重要/" title="[Day 1] 揭開模型的神秘面紗：為何XAI對機器學習如此重要？">
      [Day 1] 揭開模型的神秘面紗：為何XAI對機器學習如此重要？
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../2.從黑盒到透明化:XAI技術的發展之路/" title="[Day 2] 從黑盒到透明化：XAI技術的發展之路">
      [Day 2] 從黑盒到透明化：XAI技術的發展之路
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../3.機器學習中的可解釋性指標/" title="[Day 3] 機器學習中的可解釋性指標">
      [Day 3] 機器學習中的可解釋性指標
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../4.LIME vs SHAP:哪種XAI解釋方法更適合你/" title="[Day 4] LIME vs. SHAP：哪種XAI解釋方法更適合你？">
      [Day 4] LIME vs. SHAP：哪種XAI解釋方法更適合你？
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../5.淺談XAI與傳統機器學習的區別/" title="[Day 5] 淺談XAI與傳統機器學習的區別">
      [Day 5] 淺談XAI與傳統機器學習的區別
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" id="nav-2" type="checkbox"/>
<label class="md-nav__link" for="nav-2">
      2.XAI在傳統機器學習中的應用
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-2">
        2.XAI在傳統機器學習中的應用
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../6.非監督學習也能做到可解釋性-探索XAI在非監督學習中的應用/" title="[Day 6] 非監督學習也能做到可解釋性？探索XAI在非監督學習中的應用">
      [Day 6] 非監督學習也能做到可解釋性？探索XAI在非監督學習中的應用
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../7.KNN與XAI:從鄰居中找出模型的決策邏輯/" title="[Day 7] KNN與XAI：從鄰居中找出模型的決策邏輯">
      [Day 7] KNN與XAI：從鄰居中找出模型的決策邏輯
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../8.解釋線性模型:探索線性迴歸和邏輯迴歸的可解釋性/" title="[Day 8] 解釋線性模型：探索線性迴歸和邏輯迴歸的可解釋性">
      [Day 8] 解釋線性模型：探索線性迴歸和邏輯迴歸的可解釋性
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../9.基於樹狀結構的XAI方法:決策樹的可解釋性/" title="[Day 9] 基於樹狀結構的XAI方法：決策樹的可解釋性">
      [Day 9] 基於樹狀結構的XAI方法：決策樹的可解釋性
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../10.Permutation Importance:從特徵重要性角度解釋整個模型行為/" title="[Day 10] Permutation Importance：從特徵重要性角度解釋整個模型行為">
      [Day 10] Permutation Importance：從特徵重要性角度解釋整個模型行為
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../11.Partial Dependence Plot:探索特徵對預測值的影響/" title="[Day 11] Partial Dependence Plot：探索特徵對預測值的影響">
      [Day 11] Partial Dependence Plot：探索特徵對預測值的影響
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" id="nav-3" type="checkbox"/>
<label class="md-nav__link" for="nav-3">
      3.XAI常用工具介紹
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-3">
        3.XAI常用工具介紹
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../12.LIME理論:如何用局部線性近似解釋黑箱模型/" title="[Day 12] LIME理論：如何用局部線性近似解釋黑箱模型">
      [Day 12] LIME理論：如何用局部線性近似解釋黑箱模型
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../13.LIME實作:實戰演練LIME解釋方法/" title="[Day 13] LIME實作：實戰演練LIME解釋方法">
      [Day 13] LIME實作：實戰演練LIME解釋方法
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../14.SHAP理論:解析SHAP解釋方法的核心/" title="[Day 14] SHAP理論：解析SHAP解釋方法的核心">
      [Day 14] SHAP理論：解析SHAP解釋方法的核心
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../15.SHAP實作:實戰演練SHAP解釋方法/" title="[Day 15] SHAP實作：實戰演練SHAP解釋方法">
      [Day 15] SHAP實作：實戰演練SHAP解釋方法
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-toggle md-nav__toggle" data-md-toggle="nav-4" id="nav-4" type="checkbox"/>
<label class="md-nav__link" for="nav-4">
      4.XAI在深度學習中的可解釋性
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-4">
        4.XAI在深度學習中的可解釋性
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../16.神經網路的可解釋性:如何理解深度學習中的黑箱模型/" title="[Day 16] 神經網路的可解釋性：如何理解深度學習中的黑箱模型？">
      [Day 16] 神經網路的可解釋性：如何理解深度學習中的黑箱模型？
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../17.解析深度神經網路:使用Deep SHAP進行模型解釋/" title="[Day 17] 解析深度神經網路：使用Deep SHAP進行模型解釋">
      [Day 17] 解析深度神經網路：使用Deep SHAP進行模型解釋
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="18.CNN:卷積深度神經網路的解釋方法/" title="[Day 18] CNN：卷積深度神經網路的解釋方法">
      [Day 18] CNN：卷積深度神經網路的解釋方法
    </a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-toggle md-nav__toggle" data-md-toggle="toc" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
        [Day 19] Perturbation-Based：如何用擾動方法解釋神經網路
      </label>
<a class="md-nav__link md-nav__link--active" href="19.Perturbation-Based:如何用擾動方法解釋神經網路/" title="[Day 19] Perturbation-Based：如何用擾動方法解釋神經網路">
      [Day 19] Perturbation-Based：如何用擾動方法解釋神經網路
    </a>
<nav class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">本頁目錄</label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#occlusion-sensitivity" title="Occlusion Sensitivity (遮擋敏感度)">
    Occlusion Sensitivity (遮擋敏感度)
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#occlusion-sensitivity_1" title="Occlusion Sensitivity 的解釋過程">
    Occlusion Sensitivity 的解釋過程
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#perturbation-based-occlusion-sensitivity" title="Perturbation-Based 方法實作 (Occlusion Sensitivity)">
    Perturbation-Based 方法實作 (Occlusion Sensitivity)
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#inception-v3" title="載入預訓練模型(Inception V3)">
    載入預訓練模型(Inception V3)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#occlusion-sensitivity_2" title="Occlusion Sensitivity 實作">
    Occlusion Sensitivity 實作
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_1" title="小結">
    小結
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#reference" title="Reference">
    Reference
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="20.Gradient-Based:利用梯度訊息解釋神經網路/" title="[Day 20] Gradient-Based：利用梯度訊息解釋神經網路">
      [Day 20] Gradient-Based：利用梯度訊息解釋神經網路
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="21.Propagation-Based:探索反向傳播法的可解釋性/" title="[Day 21] Propagation-Based：探索反向傳播法的可解釋性">
      [Day 21] Propagation-Based：探索反向傳播法的可解釋性
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="22.CAM-Based:如何解釋卷積神經網路/" title="[Day 22] CAM-Based：如何解釋卷積神經網路">
      [Day 22] CAM-Based：如何解釋卷積神經網路
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="23.Attention-Based:使用注意力機制解釋CNN模型/" title="[Day 23] Attention-Based：使用注意力機制解釋CNN模型">
      [Day 23] Attention-Based：使用注意力機制解釋CNN模型
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" id="nav-5" type="checkbox"/>
<label class="md-nav__link" for="nav-5">
      5.XAI在現實生活中的應用案例
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-5">
        5.XAI在現實生活中的應用案例
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../24.LSTM的可解釋性:解析步態分類中的時序資料/" title="[Day 24] LSTM的可解釋性：從時序資料解析人體姿態預測">
      [Day 24] LSTM的可解釋性：從時序資料解析人體姿態預測
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../25.XAI在影像處理中的瑕疵檢測:解釋卷積神經網路的運作/" title="[Day 25] XAI在影像處理中的瑕疵檢測：解釋卷積神經網路的運作">
      [Day 25] XAI在影像處理中的瑕疵檢測：解釋卷積神經網路的運作
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../26.智慧工廠製程中的鋼材缺陷檢測:運用XAI解析數值型感測器數據/" title="[Day 26] XAI在表格型資料的應用：解析智慧工廠中的鋼材缺陷">
      [Day 26] XAI在表格型資料的應用：解析智慧工廠中的鋼材缺陷
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../27.XAI在NLP中的應用:以情感分析解釋語言模型/" title="[Day 27] XAI在NLP中的應用：以情感分析解釋語言模型">
      [Day 27] XAI在NLP中的應用：以情感分析解釋語言模型
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" id="nav-6" type="checkbox"/>
<label class="md-nav__link" for="nav-6">
      6.XAI的挑戰與未來
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-6">
        6.XAI的挑戰與未來
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../28.誤差分析和對抗樣本:如何利用XAI檢測模型的弱點/" title="[Day 28] 對抗樣本的挑戰：如何利用XAI檢測模型的弱點？">
      [Day 28] 對抗樣本的挑戰：如何利用XAI檢測模型的弱點？
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../29.XAI如何影響人類對技術的信任和接受程度/" title="[Day 29] XAI如何影響人類對技術的信任和接受程度？">
      [Day 29] XAI如何影響人類對技術的信任和接受程度？
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../30.XAI未來發展方向:向更可靠的機器學習模型邁進/" title="[Day30] XAI未來發展方向：向更可靠的機器學習模型邁進">
      [Day30] XAI未來發展方向：向更可靠的機器學習模型邁進
    </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">本頁目錄</label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#occlusion-sensitivity" title="Occlusion Sensitivity (遮擋敏感度)">
    Occlusion Sensitivity (遮擋敏感度)
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#occlusion-sensitivity_1" title="Occlusion Sensitivity 的解釋過程">
    Occlusion Sensitivity 的解釋過程
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#perturbation-based-occlusion-sensitivity" title="Perturbation-Based 方法實作 (Occlusion Sensitivity)">
    Perturbation-Based 方法實作 (Occlusion Sensitivity)
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#inception-v3" title="載入預訓練模型(Inception V3)">
    載入預訓練模型(Inception V3)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#occlusion-sensitivity_2" title="Occlusion Sensitivity 實作">
    Occlusion Sensitivity 實作
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_1" title="小結">
    小結
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#reference" title="Reference">
    Reference
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content">
<article class="md-content__inner md-typeset"><a class="md-content__icon pdf-download-btn" download href="../pdf/全民瘋AI系列_探索可解釋人工智慧_v1.1.pdf" title="Download"><i class="fa fas fa-download"></i><small> PDF</small></a>
<h1 id="day-19-perturbation-based">[Day 19] Perturbation-Based：如何用擾動方法解釋神經網路</h1>
<p>範例程式：<a href="https://colab.research.google.com/github/andy6804tw/crazyai-xai/blob/main/code/19.Perturbation-Based：如何用擾動方法解釋神經網路.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<p>如果想要了解一張圖片中哪些區域對於 CNN 神經網路的判斷結果具有影響力，可以參考基於擾動的 Perturbation-Based 方法。它有很多不同種的變形，其中最著名的是基於遮蔽擾動的方法，可以參考2014年於 Springer 發表的期刊論文：<a href="https://arxiv.org/abs/1311.2901">Visualizing and understanding convolutional networks</a>。在這篇論文中，提出反卷積(Deconvolution)的作者 Zeiler 透過遮蔽圖片的一部分來觀察模型的輸出，以確定圖片中哪些區域對於模型的分類是相對重要的。實際上，作者也運用遮蔽反卷積和反池化的方法，驗證了這種技術確實能夠提供 CNN 的可解釋性。</p>
<blockquote>
<p>參考論文：<a href="https://arxiv.org/abs/1311.2901">Visualizing and understanding convolutional networks</a></p>
</blockquote>
<p>Perturbation-Based 方法不需要動到神經網路的架構，僅需要變動輸入的圖片，並從中觀察模型輸出的統計分佈。並試圖理解在哪個位置被遮擋後，會對最終的預測結果造成影響。因此造成最大影響的地方就可以被判定成圖片中最重要的關鍵區域，因為只要 CNN 無法看見重要區域就無法萃取關鍵特徵，相對的模型就無法正確的判斷。</p>
<h2 id="occlusion-sensitivity">Occlusion Sensitivity (遮擋敏感度)</h2>
<p>這篇論文提出了一種名為「遮擋敏感度（Occlusion Sensitivity）」的方法，它透過在圖片的特定部分進行遮擋，以觀察網路中間層的情況和預測值的變化。這有助於我們更好地理解為何網路會做出某些決策。簡單來說，遮擋敏感度是指當我們遮擋圖像的特定部分時，觀察預測機率如何隨之變化，進而找出圖片中的重要區域。</p>
<p><img alt="" src="../image/img19-1.gif"/></p>
<p>下圖為論文中的實驗結果。該實驗使用三個測試範例，分別進行系統性的遮擋，然後觀察神經網路的反應以及結果的變化。在這個實驗中，首先對每個範例圖片的不同區域應用了一個灰色方塊進行遮擋並預測，然後觀察了神經網路在第五層特徵圖的活化程度(b)，以及將特徵圖的訊息視覺化，並投影回原始輸入圖像，然後將其顯示出來(c)。(d)根據灰色方塊的位置，顯示該類別的機率分佈(可以從中發現藍色區塊代表越重要)。例如第一張圖當遮擋了狗的臉時，屬於 <code>pomeranian</code> (博美犬)這個類別的機率會明顯下降，因為神經網路無法看到狗的臉。最後(e)這一部分顯示了在不同遮擋位置時，最有可能的類別標籤。例如在第一張圖中，大部分情況下最可能的標籤是 <code>pomeranian</code> ，但如果遮擋了狗的臉而沒有遮擋到球，則它可能預測為 <code>tennis ball</code> (網球)。</p>
<p><img alt="" src="../image/img19-2.png"/></p>
<blockquote>
<p>從上圖(d)可發現如果欲辨識的目標物體被遮蔽的話那分類的準確度就會大大降低。</p>
</blockquote>
<h3 id="occlusion-sensitivity_1">Occlusion Sensitivity 的解釋過程</h3>
<p>這種基於遮擋擾動的方法實作非常簡單。首先我們需要訓練一個效果良好的分類器，接著選取一張要解釋的圖片，並對該圖片的不同區域進行遮擋，同時監測模型的輸出機率。最後我們可以將 <code>(1-機率值)</code> 視為被遮擋區域的重要性程度。上述過程可以簡要概括為以下三個步驟：</p>
<ol>
<li>訓練分類器：訓練一個性能良好的分類器。</li>
<li>遮擋圖像區域：選擇要解釋的圖片。對圖片的不同區域進行遮擋，形成不同的遮擋版本。</li>
<li>監測輸出機率：觀察模型在每個遮擋版本下的輸出的預測機率。並使用(1-機率值)來評估被遮擋區域的重要性程度。</li>
</ol>
<blockquote>
<p>也可以使用(原始無遮擋影像的機率-遮擋後預測的機率)作為重要性的評估。</p>
</blockquote>
<p>這裡的遮擋我們稱之為 patch(灰色小區塊)，而每個 patch 大小可以事先設定好。假設一張大小 <code>224*224</code> 的影像，每個遮擋 patch 大小設定為 <code>56*56</code>，總共會產生 16 張不同位置遮擋的版本(可以參考文章一開始的的動圖)。我們可以從結果發現經過遮擋貓的臉部該類別的機率大幅度的下降，也可以間接證實模型真的有學到預測貓要看臉部五官。</p>
<p><img alt="" src="../image/img19-3.png"/></p>
<p>這種方法的優點在於容易實施，但缺點在於需要大量的計算資源。因此為了提高計算效率，每個 patch 不會重疊。此外 patch 的大小雖然可以隨意指定，但若重要的特徵範圍較大的話，使用小的 patch 可能會導致最後的解釋效果不佳。這強調了在選擇 patch 大小時需要考慮實際物體的大小，以確保解釋的準確性和有效性。</p>
<p>另外還記得 <a href="https://ithelp.ithome.com.tw/articles/10318087">Day1</a> 我在文中所描述的辨識貓狗的例子嗎？我們也可以試著遮擋鈴鐺的部分，觀察模型是不是有學錯辨識貓的關鍵特徵。從下圖結果，即使把鈴鐺遮起來還是成功辨識出貓了。終於不是一個鈴鐺分類器了（汗。</p>
<p><img alt="" src="../image/img19-4.png"/></p>
<h2 id="perturbation-based-occlusion-sensitivity">Perturbation-Based 方法實作 (Occlusion Sensitivity)</h2>
<p>接下來我們使用 Google 的 Inception V3 預訓練模型來預測一張圖片，並試著使用遮擋擾動的技巧解釋模型。Inception V3 以其獨特的網路結構而聞名，它採用了所謂的<code>Inception Module</code>，這是一種多分支卷積結構，可以在不同尺度和方向上捕捉圖像特徵。這種結構使得網路能夠更有效地處理各種複雜的圖像，並提高了圖像分類的性能。</p>
<p><img alt="" src="../image/img19-5.png"/></p>
<blockquote>
<p>Inception V3 論文：<a href="https://arxiv.org/abs/1512.00567">Rethinking the Inception Architecture for Computer Vision (CVPR 2016)</a></p>
</blockquote>
<h3 id="inception-v3">載入預訓練模型(Inception V3)</h3>
<p>首先使用 TensorFlow 載入 Inception V3 模型，將輸入張量(tensor)連接到預訓練的神經網路層，<code>imagenet</code> 表示使用在 ImageNet 資料集上預訓練的權重。<code>include_top=True</code> 表示輸出包括模型的最後分類層(全連接層)，此模型通常用於影像分類任務。</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.applications.inception_v3</span> <span class="kn">import</span> <span class="n">InceptionV3</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Input</span>

<span class="c1"># 建立一個輸入張量，指定圖像大小為224x224（RGB色彩通道）</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="c1"># 建立 InceptionV3 模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">InceptionV3</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">=</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s1">'imagenet'</span><span class="p">,</span> <span class="n">include_top</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
<p>接著載入一張圖像，對其進行預處理。其中 <code>np.expand_dims()</code> 的目的是將圖像轉換為模型可接受的維度，這裡將圖像包裝在一個批次(batch)中，通常是一個批次只有一張圖像。最後使用 Inception V3 模型的預處理函數 <code>preprocess_input()</code> 來處理圖像，以確保圖像的數值範圍和格式符合模型的要求。</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.applications.inception_v3</span> <span class="kn">import</span> <span class="n">preprocess_input</span>

<span class="c1"># 載入圖像</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="s1">'./dataset/cat_dog.jpg'</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">img_to_array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span> <span class="c1"># 將載入的圖像轉換為數組形式</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 將圖像轉換為模型可接受的維度</span>
<span class="c1"># 預處理圖像</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">preprocess_input</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
<p>確認輸入影像都完成處理過後，就可以使用已建立的 Inception V3 模型進行圖像分類預測，返回分類機率。最後再使用 <code>decode_predictions()</code> 解析取得預測結果，並取得類別名稱和相對應的預測機率。<code>pred_class_idx</code> 則是預測的標籤索引，最後模型解釋會需要用到它。</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.applications.inception_v3</span> <span class="kn">import</span> <span class="n">decode_predictions</span>

<span class="c1"># 進行圖像分類預測</span>
<span class="n">pred_proba</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># 返回分類機率</span>
<span class="c1"># 解析預測結果</span>
<span class="n">pred_class_idx</span> <span class="o">=</span> <span class="n">pred_proba</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># 找到具有最高機率的類別索引</span>
<span class="n">pred_class</span> <span class="o">=</span> <span class="n">decode_predictions</span><span class="p">(</span><span class="n">pred_proba</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># 解析取得預測結果</span>
</pre></div>
<p>我們先來看看模型預測的結果。雖然這張影像同時有一隻貓和狗，但模型在神經網路中先抓取到狗的重要特徵(例如：鼻子、嘴巴)，因此最終模型預測 <code>bull_mastiff</code>(鬥牛獒)，該類別的機率值有 98% 這麼高。</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'uint8'</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">predicted_class_name</span> <span class="o">=</span> <span class="n">pred_class</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Prediction: </span><span class="si">{</span><span class="n">predicted_class_name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">pred_class</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<p><img alt="" src="../image/img19-6.png"/></p>
<h3 id="occlusion-sensitivity_2">Occlusion Sensitivity 實作</h3>
<p>以下實現 Occlusion Sensitivity 方法，它對原始影像依序插入灰色方塊(patch)，然後遍歷這些方塊並計算它們對於模型預測的影響，最後生成 sensitivity_map 和 coordinates 以供進一步分析使用。由於影像大小為<code>224*224</code>，而每個 patch 大小 <code>75*75</code> 因此總共會生成 9 張圖並儲存在 patches。</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span> <span class="c1"># 批次大小，設定模型一次預測可以讀取幾張照片</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="mi">75</span> <span class="c1"># 方形灰色方塊大小</span>
<span class="n">target_class_idx</span> <span class="o">=</span> <span class="n">pred_class_idx</span> <span class="c1"># 預測目標的標籤索引</span>

<span class="c1"># 定義一個函數，用於將灰色方塊(patch)置換到原始影像的指定位置</span>
<span class="k">def</span> <span class="nf">apply_grey_patch</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">top_left_x</span><span class="p">,</span> <span class="n">top_left_y</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">):</span>
    <span class="n">patched_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># 置換指定區域的像素值為灰色（127.5），達到遮蔽的效果</span>
    <span class="n">patched_image</span><span class="p">[</span>
        <span class="n">top_left_y</span> <span class="p">:</span> <span class="n">top_left_y</span> <span class="o">+</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">top_left_x</span> <span class="p">:</span> <span class="n">top_left_x</span> <span class="o">+</span> <span class="n">patch_size</span><span class="p">,</span> <span class="p">:</span>
    <span class="p">]</span> <span class="o">=</span> <span class="mf">127.5</span>

    <span class="k">return</span> <span class="n">patched_image</span>

<span class="c1"># 用於記錄不同區域對於模型預測的影響(初始化為0)</span>
<span class="n">sensitivity_map</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
    <span class="p">(</span>
        <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">patch_size</span><span class="p">),</span>
        <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">patch_size</span><span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="c1"># 儲存所有遮擋的圖像</span>
<span class="n">patches</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">apply_grey_patch</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">top_left_x</span><span class="p">,</span> <span class="n">top_left_y</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">index_x</span><span class="p">,</span> <span class="n">top_left_x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">patch_size</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">index_y</span><span class="p">,</span> <span class="n">top_left_y</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">patch_size</span><span class="p">))</span>
<span class="p">]</span>
<span class="c1"># 建立一個坐標列表，用於記錄不同區域的坐標</span>
<span class="n">coordinates</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="n">index_y</span><span class="p">,</span> <span class="n">index_x</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">index_x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
        <span class="n">sensitivity_map</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">index_y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
        <span class="n">sensitivity_map</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">)</span>
<span class="p">]</span>
</pre></div>
<p>上面程式為整個流程的前置作業，先把遮擋的圖像與原圖合成，並將九張不同遮擋位置的圖片儲存在 patches，接著初始化 sensitivity_map 為0，以及生成一個座標網格 coordinates 以利於後續實作。</p>
<p><img alt="" src="../image/img19-7.png"/></p>
<blockquote>
<p>patch_size 的大小會影響解釋的結果，各位可以嘗試變動大小並觀察。</p>
</blockquote>
<p>遮擋圖片都已準備好後，接著觀察每個方塊的不同位置遮擋對於模型預測的影響。我們將影像進行前處理，接著餵入先前已建立好的 Inception V3 模型，然後提取特定類別的預測機率值，在本範例中我們要觀察 <code>bull_mastiff</code> 類別的機率，最終這些機率值存儲在 target_class_probs 中供後續使用。</p>
<div class="codehilite"><pre><span></span><span class="c1"># 建立模型輸入資料</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">patches</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># 預處理圖像</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">preprocess_input</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="c1"># 進行圖像分類預測</span>
<span class="n">pred_proba</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="c1"># 取得 bull_mastiff 類別的機率</span>
<span class="n">target_class_probs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">prob</span><span class="p">[</span><span class="n">target_class_idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">pred_proba</span>
<span class="p">]</span>
</pre></div>
<p>我們將剛剛計算出來的結果透過視覺化方式呈現。可以清楚的觀察每一張遮擋版本的圖像，以及相對應的類別機率為多少。</p>
<div class="codehilite"><pre><span></span><span class="c1"># 計算網格大小，繪圖排版用</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">patch_size</span><span class="p">)</span>
<span class="c1"># 建立一張圖包含多個子圖，並設定圖像的大小和排列方式</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
                       <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">'xticks'</span><span class="p">:[],</span> <span class="s1">'yticks'</span><span class="p">:[]},</span>
                       <span class="n">gridspec_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="c1"># 顯示每張遮擋版本的圖像</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">patches</span><span class="p">)):</span>
    <span class="c1"># 設定每個子圖像的標題，包括預測的類別名稱和相應的機率值</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">%</span><span class="n">grid</span><span class="p">,</span> <span class="n">i</span><span class="o">//</span><span class="n">grid</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Prediction: </span><span class="si">{</span><span class="n">predicted_class_name</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">target_class_probs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.6f</span><span class="si">}</span><span class="s1">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="c1"># 顯示子圖像，並將像素值除以255以將其正規化到0到1之間</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">%</span><span class="n">grid</span><span class="p">,</span> <span class="n">i</span><span class="o">//</span><span class="n">grid</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">patches</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>
</pre></div>
<p><img alt="" src="../image/img19-8.png"/></p>
<p>上一步驟已經觀察模型在每個遮擋版本下的輸出的預測機率。接下來計算每個子圖像的重要性（敏感程度）。我們使用1減去模型預測機率值，當成是那塊 patch 的重要性，這樣使得模型輸出較高的機率值將對應較低的重要性，反之亦然。因此相減之後的值越高，表示該遮擋灰色的區域越重要。最後將 sensitivity_map 的大小調整為與原始圖像相同的大小，每個像素將對應到原始圖像的相應位置，使得它可以與原始圖像一起使用或顯示。</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">matplotlib.cm</span> <span class="k">as</span> <span class="nn">cm</span>

<span class="c1"># 將 (1-機率值) 當成是那塊 patch 的重要性</span>
<span class="k">for</span> <span class="p">(</span><span class="n">index_y</span><span class="p">,</span> <span class="n">index_x</span><span class="p">),</span> <span class="n">confidence</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">coordinates</span><span class="p">,</span> <span class="n">target_class_probs</span><span class="p">):</span>
    <span class="n">sensitivity_map</span><span class="p">[</span><span class="n">index_y</span><span class="p">,</span> <span class="n">index_x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">confidence</span>
<span class="c1"># 調整為與原始圖像相同的大小</span>
<span class="n">sensitivity_maps</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">sensitivity_map</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>
<blockquote>
<p>重要性計算有很多種方法，也可以使用(原始無遮擋影像的機率-遮擋後預測的機率)。</p>
</blockquote>
<p><img alt="" src="../image/img19-9.png"/></p>
<p>每個區塊重要性都已經計算出來了，最後我們就 matplotlib 來視覺化結果吧！從結果可以發現，當我們遮擋狗的臉部會大幅度降低預判 <code>bull_mastiff</code> 類別的機率值。因此相對的可以得知辨識狗的關鍵特徵在於臉部特徵與四肢。</p>
<div class="codehilite"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">squeeze</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                        <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">'xticks'</span><span class="p">:[],</span> <span class="s1">'yticks'</span><span class="p">:[]})</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Original image'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Sensitivity maps'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">sensitivity_maps</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Overlay'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">sensitivity_maps</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
</pre></div>
<p><img alt="" src="../image/img19-10.png"/></p>
<p>最後我們再進行一個有趣的實驗，如果我們遮住狗的臉，然後再次使用 InceptionV3 模型進行預測，你猜猜會得到什麼輸出？果然如預期，模型注意力轉向貓的特徵，最終輸出了 <code>tabby</code>（虎斑貓）。</p>
<p><img alt="" src="../image/img19-11.png"/></p>
<h2 id="_1">小結</h2>
<p>Perturbation-Based 還有許多不同的方法，例如我們可以自定義目標函數以及圖片遮擋的技巧(模糊化、數值替換、加入雜訊)。並針對一個模型每個輸入去尋找遮蔽哪裡可以獲得最佳的解釋性。也就是遮蔽了哪些地方，對模型模型分類的結果會造成最大的負影響。有興趣的人也可以參考這篇論文：<a href="https://arxiv.org/abs/1704.03296">Interpretable Explanations of Black Boxes by Meaningful Perturbation</a>。</p>
<p><img alt="" src="../image/img19-12.png"/></p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://meetonfriday.com/posts/3013fdb9/#Experiments">[論文速速讀]Visualizing and Understanding Convolutional Networks</a></li>
<li><a href="https://cloud.google.com/tpu/docs/inception-v3-advanced">Google Cloud: Advanced Guide to Inception v3</a></li>
<li><a href="https://datahacker.rs/028-visualization-and-understanding-of-convolutional-neural-networks-in-pytorch/">Visualization of Convolutional Neural Networks in PyTorch</a></li>
<li><a href="https://medium.com/ai-academy-taiwan/%E5%8F%AF%E8%A7%A3%E9%87%8B-ai-xai-%E7%B3%BB%E5%88%97-01-%E5%9F%BA%E6%96%BC%E9%81%AE%E6%93%8B%E7%9A%84%E6%96%B9%E6%B3%95-perturbation-based-40899ba7e903">可解釋 AI (XAI) 系列 — 01 基於遮擋的方法 (Perturbation-Based): Occlusion Sensitivity, Meaningful Perturbation</a></li>
</ul>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-nav">
<nav class="md-footer-nav__inner md-grid">
<a class="md-flex md-footer-nav__link md-footer-nav__link--prev" href="18.CNN:卷積深度神經網路的解釋方法/" rel="prev" title="[Day 18] CNN：卷積深度神經網路的解釋方法">
<div class="md-flex__cell md-flex__cell--shrink">
<i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
</div>
<div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
<span class="md-flex__ellipsis">
<span class="md-footer-nav__direction">
                  上一頁
                </span>
                [Day 18] CNN：卷積深度神經網路的解釋方法
              </span>
</div>
</a>
<a class="md-flex md-footer-nav__link md-footer-nav__link--next" href="20.Gradient-Based:利用梯度訊息解釋神經網路/" rel="next" title="[Day 20] Gradient-Based：利用梯度訊息解釋神經網路">
<div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
<span class="md-flex__ellipsis">
<span class="md-footer-nav__direction">
                  下一頁
                </span>
                [Day 20] Gradient-Based：利用梯度訊息解釋神經網路
              </span>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
</div>
</a>
</nav>
</div>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-footer-copyright">
<div class="md-footer-copyright__highlight">
            Copyright © 2023 - 2024 10程式中
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
</div>
</div>
</div>
</footer>
</div>
<script src="../assets/javascripts/application.245445c6.js"></script>
<script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
<script src="../assets/javascripts/lunr/tinyseg.js"></script>
<script src="../assets/javascripts/lunr/lunr.ja.js"></script>
<script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
<script src="../javascripts/extra.js"></script>
<script src="../javascripts/analytics.js"></script>
</body>
</html>