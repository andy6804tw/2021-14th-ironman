
<!DOCTYPE html>

<html class="no-js" lang="zh-Hant">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="ie=edge" http-equiv="x-ua-compatible"/>
<meta content="10程式中" name="author"/>
<meta content="複製" name="lang:clipboard.copy"/>
<meta content="已複製" name="lang:clipboard.copied"/>
<meta content="ja" name="lang:search.language"/>
<meta content="True" name="lang:search.pipeline.stopwords"/>
<meta content="True" name="lang:search.pipeline.trimmer"/>
<meta content="沒有符合的項目" name="lang:search.result.none"/>
<meta content="找到 1 個符合的項目" name="lang:search.result.one"/>
<meta content="找到 # 個符合的項目" name="lang:search.result.other"/>
<meta content="[\uff0c\u3002]+" name="lang:search.tokenizer"/>
<link href="assets/images/favicon.png" rel="shortcut icon"/>
<meta content="mkdocs-1.0.4, mkdocs-material-4.4.0" name="generator"/>
<title>[Day 24] LSTM的可解釋性:從時序資料解析人體姿態預測 - 全民瘋AI系列 [探索可解釋人工智慧]</title>
<link href="assets/stylesheets/application.0284f74d.css" rel="stylesheet"/>
<link href="assets/stylesheets/application-palette.01803549.css" rel="stylesheet"/>
<meta content="#7e57c2" name="theme-color"/>
<script src="assets/javascripts/modernizr.74668098.js"></script>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&amp;display=fallback" rel="stylesheet"/>
<style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
<link href="assets/fonts/material-icons.css" rel="stylesheet"/>
<link href="stylesheets/extra.css" rel="stylesheet"/>
</head>
<body data-md-color-accent="deep-purple" data-md-color-primary="deep-purple" dir="ltr">
<svg class="md-svg">
<defs>
<svg height="448" id="__github" viewbox="0 0 416 448" width="416" xmlns="http://www.w3.org/2000/svg"><path d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z" fill="currentColor"></path></svg>
</defs>
</svg>
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
<a class="md-skip" href="#day-24-lstm" tabindex="1">
        跳轉到
      </a>
<header class="md-header" data-md-component="header">
<nav class="md-header-nav md-grid">
<div class="md-flex">
<div class="md-flex__cell md-flex__cell--shrink">
<a class="md-header-nav__button md-logo" href="." title="全民瘋AI系列 [探索可解釋人工智慧]">
<i class="md-icon"></i>
</a>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
</div>
<div class="md-flex__cell md-flex__cell--stretch">
<div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
<span class="md-header-nav__topic">
              全民瘋AI系列 [探索可解釋人工智慧]
            </span>
<span class="md-header-nav__topic">
              
                [Day 24] LSTM的可解釋性:從時序資料解析人體姿態預測
              
            </span>
</div>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="query" data-md-state="active" name="query" placeholder="搜尋" spellcheck="false" type="text"/>
<label class="md-icon md-search__icon" for="__search"></label>
<button class="md-icon md-search__icon" data-md-component="reset" tabindex="-1" type="reset">
        
      </button>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="result">
<div class="md-search-result__meta">
            打字進行搜尋
          </div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<div class="md-header-nav__source">
<a class="md-source" data-md-source="github" href="https://github.com/andy6804tw/2020-12th-ironman" title="前往倉庫">
<div class="md-source__icon">
<svg height="24" viewbox="0 0 24 24" width="24">
<use height="24" width="24" xlink:href="#__github"></use>
</svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
</div>
</div>
</nav>
</header>
<div class="md-container">
<main class="md-main">
<div class="md-main__inner md-grid" data-md-component="container">
<div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title md-nav__title--site" for="__drawer">
<a class="md-nav__button md-logo" href="." title="全民瘋AI系列 [探索可解釋人工智慧]">
<i class="md-icon"></i>
</a>
    全民瘋AI系列 [探索可解釋人工智慧]
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-source="github" href="https://github.com/andy6804tw/2020-12th-ironman" title="前往倉庫">
<div class="md-source__icon">
<svg height="24" viewbox="0 0 24 24" width="24">
<use height="24" width="24" xlink:href="#__github"></use>
</svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-toggle md-nav__toggle" data-md-toggle="nav-1" id="nav-1" type="checkbox"/>
<label class="md-nav__link" for="nav-1">
      1. XAI 基礎與概念介紹
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-1">
        1. XAI 基礎與概念介紹
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="1. 揭開模型的神秘面紗:為何XAI對機器學習如此重要.html" title="[Day 1] 揭開模型的神秘面紗:為何XAI對機器學習如此重要?">
      [Day 1] 揭開模型的神秘面紗:為何XAI對機器學習如此重要?
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="2. 從黑盒到透明化:XAI技術的發展之路.html" title="[Day 2] 從黑盒到透明化:XAI技術的發展之路">
      [Day 2] 從黑盒到透明化:XAI技術的發展之路
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="3. 機器學習中的可解釋性指標.html" title="[Day 3] 機器學習中的可解釋性指標">
      [Day 3] 機器學習中的可解釋性指標
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="4. LIME vs SHAP:哪種XAI解釋方法更適合你.html" title="[Day 4] LIME vs. SHAP:哪種XAI解釋方法更適合你?">
      [Day 4] LIME vs. SHAP:哪種XAI解釋方法更適合你?
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="5. 淺談XAI與傳統機器學習的區別.html" title="[Day 5] 淺談XAI與傳統機器學習的區別">
      [Day 5] 淺談XAI與傳統機器學習的區別
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" id="nav-2" type="checkbox"/>
<label class="md-nav__link" for="nav-2">
      2. XAI 在傳統機器學習中的應用
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-2">
        2. XAI 在傳統機器學習中的應用
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="6. 非監督學習也能做到可解釋性-探索XAI在非監督學習中的應用.html" title="[Day 6] 非監督學習也能做到可解釋性?探索XAI在非監督學習中的應用">
      [Day 6] 非監督學習也能做到可解釋性?探索XAI在非監督學習中的應用
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="7. KNN與XAI:從鄰居中找出模型的決策邏輯.html" title="[Day 7] KNN與XAI:從鄰居中找出模型的決策邏輯">
      [Day 7] KNN與XAI:從鄰居中找出模型的決策邏輯
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="8. 解釋線性模型:探索線性迴歸和邏輯迴歸的可解釋性.html" title="[Day 8] 解釋線性模型:探索線性迴歸和邏輯迴歸的可解釋性">
      [Day 8] 解釋線性模型:探索線性迴歸和邏輯迴歸的可解釋性
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="9. 基於樹狀結構的XAI方法:決策樹的可解釋性.html" title="[Day 9] 基於樹狀結構的XAI方法:決策樹的可解釋性">
      [Day 9] 基於樹狀結構的XAI方法:決策樹的可解釋性
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="10. Permutation Importance:從特徵重要性角度解釋整個模型行為.html" title="[Day 10] Permutation Importance:從特徵重要性角度解釋整個模型行為">
      [Day 10] Permutation Importance:從特徵重要性角度解釋整個模型行為
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="11. Partial Dependence Plot:探索特徵對預測值的影響.html" title="[Day 11] Partial Dependence Plot:探索特徵對預測值的影響">
      [Day 11] Partial Dependence Plot:探索特徵對預測值的影響
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" id="nav-3" type="checkbox"/>
<label class="md-nav__link" for="nav-3">
      3. XAI 常用工具介紹
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-3">
        3. XAI 常用工具介紹
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="12. LIME理論:如何用局部線性近似解釋黑箱模型.html" title="[Day 12] LIME理論:如何用局部線性近似解釋黑箱模型">
      [Day 12] LIME理論:如何用局部線性近似解釋黑箱模型
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="13. LIME實作:實戰演練LIME解釋方法.html" title="[Day 13] LIME實作:實戰演練LIME解釋方法">
      [Day 13] LIME實作:實戰演練LIME解釋方法
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="14. SHAP理論:解析SHAP解釋方法的核心.html" title="[Day 14] SHAP理論:解析SHAP解釋方法的核心">
      [Day 14] SHAP理論:解析SHAP解釋方法的核心
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="15. SHAP實作:實戰演練SHAP解釋方法.html" title="[Day 15] SHAP實作:實戰演練SHAP解釋方法">
      [Day 15] SHAP實作:實戰演練SHAP解釋方法
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" id="nav-4" type="checkbox"/>
<label class="md-nav__link" for="nav-4">
      4. XAI 在深度學習中的可解釋性
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-4">
        4. XAI 在深度學習中的可解釋性
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="16. 神經網路的可解釋性:如何理解深度學習中的黑箱模型.html" title="[Day 16] 神經網路的可解釋性:如何理解深度學習中的黑箱模型?">
      [Day 16] 神經網路的可解釋性:如何理解深度學習中的黑箱模型?
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="17.  解析深度神經網路:使用Deep SHAP進行模型解釋.html" title="[Day 17] 解析深度神經網路:使用Deep SHAP進行模型解釋">
      [Day 17] 解析深度神經網路:使用Deep SHAP進行模型解釋
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="18. CNN:卷積深度神經網路的解釋方法.html" title="[Day 18] CNN:卷積深度神經網路的解釋方法">
      [Day 18] CNN:卷積深度神經網路的解釋方法
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="19. Perturbation-Based:如何用擾動方法解釋神經網路.html" title="[Day 19] Perturbation-Based:如何用擾動方法解釋神經網路">
      [Day 19] Perturbation-Based:如何用擾動方法解釋神經網路
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="20. Gradient-Based:利用梯度訊息解釋神經網路.html" title="[Day 20] Gradient-Based:利用梯度訊息解釋神經網路">
      [Day 20] Gradient-Based:利用梯度訊息解釋神經網路
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="21. Propagation-Based:探索反向傳播法的可解釋性.html" title="[Day 21] Propagation-Based:探索反向傳播法的可解釋性">
      [Day 21] Propagation-Based:探索反向傳播法的可解釋性
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="22. CAM-Based:如何解釋卷積神經網路.html" title="[Day 22] CAM-Based:如何解釋卷積神經網路">
      [Day 22] CAM-Based:如何解釋卷積神經網路
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="23. Attention-Based:使用注意力機制解釋CNN模型.html" title="[Day 23] Attention-Based:使用注意力機制解釋CNN模型">
      [Day 23] Attention-Based:使用注意力機制解釋CNN模型
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-toggle md-nav__toggle" data-md-toggle="nav-5" id="nav-5" type="checkbox"/>
<label class="md-nav__link" for="nav-5">
      5. XAI 在現實生活中的應用案例
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-5">
        5. XAI 在現實生活中的應用案例
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active">
<input class="md-toggle md-nav__toggle" data-md-toggle="toc" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
        [Day 24] LSTM的可解釋性:從時序資料解析人體姿態預測
      </label>
<a class="md-nav__link md-nav__link--active" href="24. LSTM的可解釋性:解析步態分類中的時序資料.html" title="[Day 24] LSTM的可解釋性:從時序資料解析人體姿態預測">
      [Day 24] LSTM的可解釋性:從時序資料解析人體姿態預測
    </a>
<nav class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">本頁目錄</label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#mobile-health-human-behavior-analysis" title="[實作] Mobile Health Human Behavior Analysis">
    [實作] Mobile Health Human Behavior Analysis
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_1" title="載入資料集">
    載入資料集
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_2" title="資料預處理">
    資料預處理
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lstm" title="LSTM 模型建立">
    LSTM 模型建立
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#deep-shap-lstm" title="Deep SHAP 解釋 LSTM 模型">
    Deep SHAP 解釋 LSTM 模型
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#shap-summary-plot" title="SHAP Summary Plot (全局解釋)">
    SHAP Summary Plot (全局解釋)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#shap-force-plot" title="SHAP Force plot (單筆資料解釋)">
    SHAP Force plot (單筆資料解釋)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#shap-waterfall-plot" title="SHAP waterfall plot (單筆資料解釋)">
    SHAP waterfall plot (單筆資料解釋)
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_3" title="小結">
    小結
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#reference" title="Reference">
    Reference
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="25. XAI在影像處理中的瑕疵檢測:解釋卷積神經網路的運作.html" title="[Day 25] XAI在影像處理中的瑕疵檢測:解釋卷積神經網路的運作">
      [Day 25] XAI在影像處理中的瑕疵檢測:解釋卷積神經網路的運作
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="26. 智慧工廠製程中的鋼材缺陷檢測:運用XAI解析數值型感測器數據.html" title="[Day 26] XAI在表格型資料的應用:解析智慧工廠中的鋼材缺陷">
      [Day 26] XAI在表格型資料的應用:解析智慧工廠中的鋼材缺陷
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="27. XAI在NLP中的應用:以情感分析解釋語言模型.html" title="[Day 27] XAI在NLP中的應用:以情感分析解釋語言模型">
      [Day 27] XAI在NLP中的應用:以情感分析解釋語言模型
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" id="nav-6" type="checkbox"/>
<label class="md-nav__link" for="nav-6">
      6. XAI 的挑戰與未來
    </label>
<nav class="md-nav" data-md-component="collapsible" data-md-level="1">
<label class="md-nav__title" for="nav-6">
        6. XAI 的挑戰與未來
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="28. 誤差分析和對抗樣本:如何利用XAI檢測模型的弱點.html" title="[Day 28] XAI如何影響人類對技術的信任和接受程度?">
      [Day 28] XAI如何影響人類對技術的信任和接受程度?
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="29. XAI如何影響人類對技術的信任和接受程度.html" title="[Day 29] 對抗樣本的挑戰:如何利用XAI檢測模型的弱點?">
      [Day 29] 對抗樣本的挑戰:如何利用XAI檢測模型的弱點?
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="30. XAI未來發展方向:向更可靠的機器學習模型邁進.html" title="[Day 30] XAI未來發展方向:向更可靠的機器學習模型邁進">
      [Day 30] XAI未來發展方向:向更可靠的機器學習模型邁進
    </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">本頁目錄</label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#mobile-health-human-behavior-analysis" title="[實作] Mobile Health Human Behavior Analysis">
    [實作] Mobile Health Human Behavior Analysis
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_1" title="載入資料集">
    載入資料集
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_2" title="資料預處理">
    資料預處理
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#lstm" title="LSTM 模型建立">
    LSTM 模型建立
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#deep-shap-lstm" title="Deep SHAP 解釋 LSTM 模型">
    Deep SHAP 解釋 LSTM 模型
  </a>
<nav class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#shap-summary-plot" title="SHAP Summary Plot (全局解釋)">
    SHAP Summary Plot (全局解釋)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#shap-force-plot" title="SHAP Force plot (單筆資料解釋)">
    SHAP Force plot (單筆資料解釋)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#shap-waterfall-plot" title="SHAP waterfall plot (單筆資料解釋)">
    SHAP waterfall plot (單筆資料解釋)
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_3" title="小結">
    小結
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#reference" title="Reference">
    Reference
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content">
<article class="md-content__inner md-typeset"><a class="md-content__icon pdf-download-btn" download href="pdf/全民瘋AI系列_探索可解釋人工智慧_v1.1.pdf" title="Download"><i class="fa fas fa-download"></i><small> PDF</small></a>
<h1 id="day-24-lstm">[Day 24] LSTM的可解釋性：從時序資料解析人體姿態預測</h1>
<p>在這個系列中，我們已經介紹了深度神經網路中的DNN（深度神經網路）和CNN（卷積神經網路），以及它們如何透過不同的方法進行模型解釋。今天，我們將深入探討如何使用 Deep SHAP 方法來解釋長短期記憶網路（LSTM）模型。LSTM 是一種特殊類型的循環神經網路（RNN），用於處理時間序列資料以及具有長期依賴性的序列任務。LSTM 的主要特點是它能夠有效地捕捉和記憶過去的訊息，以便在處理未來的時間步時進行預測。</p>
<p>LSTM 之所以強大，是因為它具有以下關鍵結構和機制：
- Cell State： LSTM具有記憶單元，可以存儲和檢索過去的訊息，這使得它能夠處理長序列資料，並保持對序列中先前訊息的適當記憶。
- Hidden State：LSTM還具有一個隱藏狀態，它是根據當前輸入和先前的隱藏狀態計算而來。隱藏狀態包含了當前時間步的信息，並用於生成預測。
- 三個控制門(Gate)：LSTM包含三個訊息控制單元的結構，分別是遺忘門（Forget Gate）、輸入門（Input Gate）和輸出門（Output Gate），通常會搭配Cell State和Hidden State控制輸出訊息。</p>
<p><img alt="" src="image/img24-1.png"/></p>
<blockquote>
<p>Deep SHAP 介紹可以參考：<a href="https://ithelp.ithome.com.tw/articles/10331443">[Day 17] 解析深度神經網路：使用Deep SHAP進行模型解釋</a></p>
</blockquote>
<h2 id="mobile-health-human-behavior-analysis">[實作] Mobile Health Human Behavior Analysis</h2>
<p>在今日的範例中我們將採用 <a href="http://archive.ics.uci.edu/dataset/319/mhealth+dataset">Mobile HEALTH</a> 公開資料集，以建立 LSTM 時間序列模型，並用於預測人體動作辨識。該資料集包含了十名來自不同族群的志願者，在參與各項活動時，記錄了他們的外部感測器數值和生理訊號。接下來的實作中，我們僅使用了其中一位受試者的左腳踝三軸加速度（x、y、z）和陀螺儀（x、y、z）的資料，總共有六項特徵。這些資料以每秒50次的取樣率進行記錄(50Hz)，每種不同的動作分別持續了一分鐘，包括以下五種姿態：</p>
<ul>
<li>L1: 站立 (1 min)</li>
<li>L2: 靜坐 (1 min)</li>
<li>L3: 平躺 (1 min)</li>
<li>L4: 走路 (1 min)</li>
<li>L5: 上樓 (1 min)</li>
</ul>
<p><img alt="" src="image/img24-2.png"/></p>
<p>從下圖中，我們可以觀察前500個資料點（約為10秒間隔）的訊號變化情況。在這段時間內，我們可以觀察到站立、靜坐和平躺等狀態的訊號變化相對較平穩，但仍可以通過旋轉角度和方位的變化來進行辨識。此外走路和上樓等動作則展現出明顯的訊號週期性，這些特徵可以被用來進行動作辨識。</p>
<p><img alt="" src="image/img24-3.png"/></p>
<blockquote>
<p>原始資料單位：加速度(m/s^2), 陀螺儀角速度(deg/s)</p>
</blockquote>
<h2 id="_1">載入資料集</h2>
<p>首先我們透過 pandas 套件讀取一位受試者的訓練資料。這一份 csv 檔案中共有 161280 筆資料，其中包含數入訊號以及相對應標籤共有7個欄位。</p>
<ul>
<li>輸入特徵<ul>
<li>acc_l_ankle_x(左腳踝加速度x)</li>
<li>acc_l_ankle_y(左腳踝加速度y)</li>
<li>acc_l_ankle_z(左腳踝加速度z)</li>
</ul>
</li>
<li>輸出標籤<ul>
<li>Label 0~12 (今日範例只拿L1~L5)</li>
</ul>
</li>
</ul>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">df_data</span> <span class="o">=</span>  <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'https://github.com/andy6804tw/2023-15th-ironman/raw/main/dataset/mHealth_subject1.csv'</span><span class="p">)</span>
<span class="n">x_feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'acc_l_ankle_x'</span><span class="p">,</span><span class="s1">'acc_l_ankle_y'</span><span class="p">,</span><span class="s1">'acc_l_ankle_z'</span><span class="p">,</span><span class="s1">'gyro_l_ankle_x'</span><span class="p">,</span><span class="s1">'gyro_l_ankle_y'</span><span class="p">,</span><span class="s1">'gyro_l_ankle_z'</span><span class="p">]</span>
<span class="n">y_feature_name</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'label'</span><span class="p">]</span>
<span class="n">y_label_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'站立'</span><span class="p">,</span> <span class="s1">'靜坐'</span><span class="p">,</span> <span class="s1">'平躺'</span><span class="p">,</span> <span class="s1">'走路'</span><span class="p">,</span> <span class="s1">'上樓'</span><span class="p">]</span>
<span class="n">df_data</span> <span class="o">=</span> <span class="n">df_data</span><span class="p">[</span><span class="n">x_feature_names</span> <span class="o">+</span> <span class="n">y_feature_name</span><span class="p">]</span>
</pre></div>
<p><img alt="" src="image/img24-4.png"/></p>
<p>這一份資料集總共有 L0~L12 共13種標籤各自代表不同的姿態，其中在今天的範例中我們僅從資料集中提取 L1~L5 五種類別，依序分別代表<code>站立</code>、<code>靜坐</code>、<code>平躺</code>、<code>走路</code>、<code>上樓</code>。</p>
<div class="codehilite"><pre><span></span><span class="n">L1</span> <span class="o">=</span> <span class="n">df_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span> <span class="n">df_data</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="c1">#L1: 站立 (1 min) </span>
<span class="n">L2</span> <span class="o">=</span> <span class="n">df_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span> <span class="n">df_data</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">]</span> <span class="c1">#L2: 靜坐 (1 min)</span>
<span class="n">L3</span> <span class="o">=</span> <span class="n">df_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span> <span class="n">df_data</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">3</span><span class="p">]</span> <span class="c1">#L3: 平躺 (1 min)</span>
<span class="n">L4</span> <span class="o">=</span> <span class="n">df_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span> <span class="n">df_data</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">4</span><span class="p">]</span> <span class="c1">#L4: 走路 (1 min)</span>
<span class="n">L5</span> <span class="o">=</span> <span class="n">df_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span> <span class="n">df_data</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">5</span><span class="p">]</span> <span class="c1">#L5: 上樓 (1 min)</span>
</pre></div>
<h2 id="_2">資料預處理</h2>
<p>這段程式碼的主要目的是根據指定的窗口大小（window_size）和間隔（shift），從給定的時間序列資料中提取觀察資料（X）和相應的預測資料（y）。每個觀察資料是一個連續時間段內的資料，而預測資料則是該時間段後的一個時間點的特定特徵值。</p>
<div class="codehilite"><pre><span></span><span class="c1"># 抓取window_size的資料作為觀察資料(x), 預測下一時間點步態(y)</span>
<span class="k">def</span> <span class="nf">window_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">shift</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># 將連續的window_size時間段內的資料（去除最後一個特徵label）作為觀察資料x</span>
        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">window_size</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="c1"># 將接下來的一個時間點的第6個特徵（標籤）作為預測資料y</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">window_size</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="n">shift</span>  <span class="c1"># 移動索引，以繼續抓取下一筆資料</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># 確保觀察資料和預測資料的數量一致</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>  <span class="c1"># 返回處理後的時序資料和預測資料</span>
</pre></div>
<p>簡單來說以站立資料採樣為例，約1分鐘3072筆的資料，每筆時序資料取視窗大小100點資料，接著移動20點再另外收集。因此每筆訓練資料的輸入維度應該為 (100,6)，100代表 window_size 也就是要看幾筆的資料點，而 6 代表不同感測器所截取的數值左腳踝加速度(x,y,z)和陀螺儀(x,y,z)。我們分別將 L1~L5 資料依據時間點進行採樣組成多筆時序資料，最後再透過 <code>np.concatenate()</code> 將所有資料合併成 X 和 y。另外需注意的是在處理 y 的時候 <code>-1</code> 操作是為了將標籤從 1、2、3、4、5 轉換為 0、1、2、3、4，使標籤從0開始編號，因為在訓練神經網路的時候分類的標籤都是從0開始。</p>
<div class="codehilite"><pre><span></span><span class="c1"># 蒐集六個訊號100個時間點(window_size)，每筆資料採樣移動20個資料點</span>
<span class="n">X_L1</span><span class="p">,</span> <span class="n">y_L1</span> <span class="o">=</span> <span class="n">window_data</span><span class="p">(</span><span class="n">L1</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_L2</span><span class="p">,</span> <span class="n">y_L2</span> <span class="o">=</span> <span class="n">window_data</span><span class="p">(</span><span class="n">L2</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_L3</span><span class="p">,</span> <span class="n">y_L3</span> <span class="o">=</span> <span class="n">window_data</span><span class="p">(</span><span class="n">L3</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_L4</span><span class="p">,</span> <span class="n">y_L4</span> <span class="o">=</span> <span class="n">window_data</span><span class="p">(</span><span class="n">L4</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_L5</span><span class="p">,</span> <span class="n">y_L5</span> <span class="o">=</span> <span class="n">window_data</span><span class="p">(</span><span class="n">L5</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X_L1</span><span class="p">,</span> <span class="n">X_L2</span><span class="p">,</span> <span class="n">X_L3</span><span class="p">,</span> <span class="n">X_L4</span><span class="p">,</span> <span class="n">X_L5</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">y_L1</span><span class="p">,</span> <span class="n">y_L2</span><span class="p">,</span> <span class="n">y_L3</span><span class="p">,</span> <span class="n">y_L4</span><span class="p">,</span> <span class="n">y_L5</span><span class="p">])</span><span class="o">-</span><span class="mi">1</span> <span class="c1"># 標籤要從0開始故全部減一</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"X shape: "</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y shape: "</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<p>輸出結果可以看到總共有 745 筆資料， window_size 為 100，每一筆時序資料共有 6 項特徵。</p>
<div class="codehilite"><pre><span></span>X shape:  (745, 100, 6)
y shape:  (745,)
</pre></div>
<p>資料準備就緒後最後一個步驟就是將 X 和 y 切割訓練集與測試集。</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"X_train shape = "</span><span class="p">,</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"X_test shape = "</span><span class="p">,</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y_train shape = "</span><span class="p">,</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y_test shape = "</span><span class="p">,</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<p>輸出結果：</p>
<div class="codehilite"><pre><span></span>X_train shape =  (670, 100, 6)
X_test shape =  (75, 100, 6)
y_train shape =  (670,)
y_test shape =  (75,)
</pre></div>
<h2 id="lstm">LSTM 模型建立</h2>
<p>以下程式碼使用 Tensorflow2.0 Functional API 搭建神經網路。此模型架構是用於處理時間序列資料並進行分類任務。模型的輸入是具有6個傳感器特徵的時間序列資料，每筆資料包含100個時間點。模型包括一個 LSTM 隱藏層，用於捕捉時間序列的時間相關性，其中 <code>return_sequences=True</code> 並將每個神經元的隱藏狀態(hidden_state)回傳，並透過 <code>Flatten()</code> 將所有時間的隱藏狀態攤平成一維向量傳給輸出層。最後通過一個全連接層進行分類，將輸入資料分為5個不同的類別。</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">tensorflow.compat.v1.keras.backend</span> <span class="kn">import</span> <span class="n">get_session</span>
<span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">disable_v2_behavior</span><span class="p">()</span>

<span class="n">num_sensor</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">window_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">def</span> <span class="nf">build_model</span><span class="p">():</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">window_size</span><span class="p">,</span> <span class="n">num_sensor</span><span class="p">))</span>
    <span class="c1"># 第一層隱藏層</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">model_input</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># 輸出層</span>
    <span class="n">model_output</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Model</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="n">model_output</span><span class="p">)</span>
</pre></div>
<blockquote>
<p>由於 SHAP 目前還尚未支援 TF2.4 版本以上，因此必須透過 <code>tf.compat.v1.disable_v2_behavior()</code> 關閉一些 2.0 版本的進階 API。</p>
</blockquote>
<p>接下來，使用先前定義的 build_model() 函數建立一個新的神經網絡模型，並將這個模型存儲在 model 變數中。最後使用 model.summary() 印出模型的摘要訊息，包括模型的結構、每一層的參數數量等。</p>
<div class="codehilite"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
<p><img alt="" src="image/img24-5.png"/></p>
<p>模型準備就緒後即可開始訓練模型。由於我們沒有針對輸出標籤進行 <code>one-hot encoding</code> 因此可以在 loss 指定 <code>sparse_categorical_crossentropy</code>，這樣在模型訓練過程中自動地會進行額外處理以利於 cross entropy 的計算。由於訓練資料沒有很多，因此批次大小設為 4 表示每次訓練過程中，模型會同時處理4筆訓練資料。訓練迭代次數為 10 次。最後使用訓練數據 X_train 和 y_train 來訓練模型。</p>
<div class="codehilite"><pre><span></span><span class="c1"># 編譯模型</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
      <span class="n">loss</span><span class="o">=</span><span class="s1">'sparse_categorical_crossentropy'</span><span class="p">,</span>
      <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>
      <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">])</span>
<span class="c1"># 訓練模型</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> 
                    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
<p><img alt="" src="image/img24-6.png"/></p>
<h2 id="deep-shap-lstm">Deep SHAP 解釋 LSTM 模型</h2>
<p>這裡採用 SHAP 套件中的 Deep SHAP 進行深度神經網路模型的解釋，它結合了 DeepLIFT 和 Shapely values 的概念，以計算每個特徵對於模型預測的貢獻。首先建立一個 DeepExplainer 解釋器，他除了 DNN 模型可以解釋之外，其他類型的神經網路像是 LSTM、CNN、1DCNN 都可以透過它來對模型進行特徵歸因的重要性解釋。另外在估計 Shapely values 時，可以輸入要解釋的資料。在這個範例中，我們將使用75筆測試集資料進行模型解釋。</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">shap</span>
<span class="n">shap</span><span class="o">.</span><span class="n">initjs</span><span class="p">()</span>

<span class="c1"># 使用 Deep SHAP 解釋模型</span>
<span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">DeepExplainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">X_train</span><span class="p">)</span>
<span class="c1"># 估計 Shapely values</span>
<span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">shap_values</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
<p>由於我們需要處理大量的測試集資料，所以輸入資料的維度為(75, 100, 6)。這裡的每個數值代表的含義分別是(資料筆數, 時間窗口大小, 特徵數)。最後我們可以觀察到，在處理時間序列資料計算 Shapely values 時，輸出的維度將是(5, 75, 100, 6)。這是因為這個模型是一個輸出五類別機率的分類任務，所以在計算 Shapely values 時，會針對每一筆資料分別計算五個類別對應的 Shapely values。</p>
<div class="codehilite"><pre><span></span><span class="c1"># 5個類別, 75筆測試資料, 100個時間點, 6個特徵</span>
<span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">shap_values</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
<p>輸出結果：</p>
<div class="codehilite"><pre><span></span>(5, 75, 100, 6)
</pre></div>
<h3 id="shap-summary-plot">SHAP Summary Plot (全局解釋)</h3>
<p>我們可以針對75筆測試資料進行每個類別的特徵重要程度排序。接著透過變數 <code>label</code> 的設定可以觀察在某標籤下每個特徵對於整體平均貢獻的值。由於 Shapely values 會針對窗口的大小進行每個時間點的特徵歸因計算。因此每筆資料在某個類別的 Shapely values 維度是二維的(100, 6)，所以在進行全局模型解釋前要手動的將每筆測試資料的時間點根據每個特徵維度進行相加得到每個特徵的貢獻程度。最後計算出來六個數值再通過一個標準化，得到在某標籤下每個特徵對於整體平均貢獻的值。</p>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">normalize_array</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="c1"># 計算陣列中所有元素的總和</span>
    <span class="n">total</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="c1"># 正規化每個元素</span>
    <span class="n">normalized_arr</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="o">/</span> <span class="n">total</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">normalized_arr</span>
</pre></div>
<div class="codehilite"><pre><span></span><span class="c1"># 獲得在某標籤下每個特徵對於整體平均貢獻的值</span>
<span class="c1"># 0:站立、1:靜坐、2:平躺、3:走路、4:上樓</span>
<span class="n">label</span><span class="o">=</span><span class="mi">0</span>
<span class="n">shap_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">shap_values</span><span class="p">)</span>
<span class="n">shap_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">shap_value</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>
<span class="n">shap_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">shap_value</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">SHAP_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">shap_value</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">shap_value</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">shap_value</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">shap_value</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">shap_value</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">shap_value</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">])]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">x_feature_names</span><span class="p">,</span> <span class="n">normalize_array</span><span class="p">(</span><span class="n">SHAP_list</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">y_label_names</span><span class="p">[</span><span class="n">label</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">'#028bfb'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
<p>我們可以參照文章一開始觀察原始訊號圖，並發現各種步態可以很清楚地透過加速度規訊號進行判斷。因此在模型的推論過程中，主要依賴於加速度資訊，具體來說是 acc_x、acc_y 和 acc_z 這三項資訊。果然在 SHAP 模型解釋中，判斷各種類別的重要性都是以那三項資訊為主要依據。另外需注意的是，在本範例中，我們尚未對所有資料進行標準化，因此各位讀者也可以嘗試對資料進行標準化，看看是否會對模型的預測結果產生不同的解釋結果。</p>
<p><img alt="" src="image/img24-7.png"/></p>
<h3 id="shap-force-plot">SHAP Force plot (單筆資料解釋)</h3>
<p>我們從資料集中選取了75筆資料作為測試集。剛才提到的全局解釋是針對這75筆資料的整體平均進行的解釋。現在我們可以進一步針對每一筆數據進行解釋分析。在下面程式中的 <code>index</code> 被設定為0，這表示我們要觀察測試集中的第一筆資料。然後，我們使用force_plot函式對這筆資料進行預測，並將分析結果以視覺化方式呈現。</p>
<div class="codehilite"><pre><span></span><span class="c1"># 觀察測試集中第一筆資料的重要程度</span>
<span class="n">index</span><span class="o">=</span><span class="mi">0</span>
<span class="n">pred_class</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[[</span><span class="n">index</span><span class="p">]])</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
<span class="n">pred_proba</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[[</span><span class="n">index</span><span class="p">]])[</span><span class="mi">0</span><span class="p">][</span><span class="n">pred_class</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'測試集第 </span><span class="si">{</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1"> 筆模型預測結果: </span><span class="si">{</span><span class="n">pred_class</span><span class="si">}</span><span class="s1"> 機率值: </span><span class="si">{</span><span class="n">pred_proba</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'真實答案: </span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="n">index</span><span class="p">])</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="n">shap_value</span> <span class="o">=</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">pred_class</span><span class="p">][</span><span class="n">index</span><span class="p">]</span>
<span class="n">shap_value</span> <span class="o">=</span> <span class="n">shap_value</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">shap</span><span class="o">.</span><span class="n">force_plot</span><span class="p">(</span><span class="n">explainer</span><span class="o">.</span><span class="n">expected_value</span><span class="p">[</span><span class="n">pred_class</span><span class="p">],</span> <span class="n">shap_value</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">x_feature_names</span><span class="p">)</span>
</pre></div>
<p>從下圖視覺化解釋結果可以看到模型在這一筆資料預測標籤 1（靜坐），其機率值為 0.99。紅色的特徵表示將正向的影響輸出機率，會使值升高，藍色的則相反，會用來降低輸出機率。</p>
<p><img alt="" src="image/img24-8.png"/></p>
<h3 id="shap-waterfall-plot">SHAP waterfall plot (單筆資料解釋)</h3>
<p>我們可以更近一步的觀察該筆資料每個特徵對於輸出某個類別的 Shapely values 數值，以及判斷該類別的基準值是多少。下圖中每個特徵相對應的 Shapely value 累加，並加上基準值 <code>E[f(x)]=0.206</code> 最終相加的結果就是該筆測試資料預測某類的的機率值。</p>
<div class="codehilite"><pre><span></span><span class="n">pred_class</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[[</span><span class="n">index</span><span class="p">]])</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
<span class="n">pred_proba</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[[</span><span class="n">index</span><span class="p">]])[</span><span class="mi">0</span><span class="p">][</span><span class="n">pred_class</span><span class="p">]</span>
<span class="n">shap_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">shap_values</span><span class="p">)</span>
<span class="n">shap_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">((</span><span class="n">shap_value</span><span class="p">[</span><span class="n">pred_class</span><span class="p">])[</span><span class="n">index</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">shap_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">shap_value</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">shap</span><span class="o">.</span><span class="n">waterfall_plot</span><span class="p">(</span><span class="n">shap</span><span class="o">.</span><span class="n">Explanation</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="n">shap_value</span><span class="p">,</span> 
                                    <span class="n">base_values</span><span class="o">=</span><span class="n">explainer</span><span class="o">.</span><span class="n">expected_value</span><span class="p">[</span><span class="n">pred_class</span><span class="p">],</span>  
                                    <span class="n">feature_names</span><span class="o">=</span><span class="n">x_feature_names</span><span class="p">))</span>
</pre></div>
<p>下圖範例是預測第一筆測試資料在標籤為1的時候的 Shapely values 數值。</p>
<p><img alt="" src="image/img24-9.png"/></p>
<blockquote>
<p>加總結果： 0.206-3.245-0.815+4.84+0.138-2.142+2.017=0.99</p>
</blockquote>
<h2 id="_3">小結</h2>
<p>個人目前觀察 SHAP 套件中的 Deep SHAP 在官方的例子中是 LSTM 自然語言的例子為主。然而在 LSTM 模型於分類或迴歸的模型，SHAP 套件無法直接進行解釋，必須透過小小的前處理透過特徵的維度將每個時間因子的 Shapely values 先加總再送進去 <code>force_plot</code> 進行解釋。此外目前套件尚未針對最新版 TensorFlow 進行優化，所以使用上會受到一些限制。如果對時間序列模型解釋有需求的讀者，也可以參考 <a href="https://arxiv.org/abs/2012.00073">TimeSHAP: Explaining Recurrent Models through Sequence Perturbations</a> 這篇研究，以及相關的實作 <a href="https://github.com/feedzai/timeshap">TimeSHAP</a> 這些資源或許會更適合處理時間序列模型的解釋需求。</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://medium.com/@sakamoto2000.kim/applied-shap-on-the-polynomial-equation-case-with-lstm-algorithm-7c140d15736b">[forecast][LSTM+SHAP]Applied SHAP on the polynomial equation case with LSTM algorithm</a></li>
</ul>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-nav">
<nav class="md-footer-nav__inner md-grid">
<a class="md-flex md-footer-nav__link md-footer-nav__link--prev" href="23. Attention-Based:使用注意力機制解釋CNN模型.html" rel="prev" title="[Day 23] Attention-Based:使用注意力機制解釋CNN模型">
<div class="md-flex__cell md-flex__cell--shrink">
<i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
</div>
<div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
<span class="md-flex__ellipsis">
<span class="md-footer-nav__direction">
                  上一頁
                </span>
                [Day 23] Attention-Based:使用注意力機制解釋CNN模型
              </span>
</div>
</a>
<a class="md-flex md-footer-nav__link md-footer-nav__link--next" href="25. XAI在影像處理中的瑕疵檢測:解釋卷積神經網路的運作.html" rel="next" title="[Day 25] XAI在影像處理中的瑕疵檢測:解釋卷積神經網路的運作">
<div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
<span class="md-flex__ellipsis">
<span class="md-footer-nav__direction">
                  下一頁
                </span>
                [Day 25] XAI在影像處理中的瑕疵檢測:解釋卷積神經網路的運作
              </span>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
</div>
</a>
</nav>
</div>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-footer-copyright">
<div class="md-footer-copyright__highlight">
            Copyright © 2023 - 2024 10程式中
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
</div>
</div>
</div>
</footer>
</div>
<script src="assets/javascripts/application.245445c6.js"></script>
<script src="assets/javascripts/lunr/lunr.stemmer.support.js"></script>
<script src="assets/javascripts/lunr/tinyseg.js"></script>
<script src="assets/javascripts/lunr/lunr.ja.js"></script>
<script>app.initialize({version:"1.0.4",url:{base:"."}})</script>
<script src="javascripts/extra.js"></script>
</body>
</html>